<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />

  <title>Kristoffer Waldow</title>

  <meta name="author" content="Kristoffer Waldow">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta content="Kristoffer Waldow, PhD Student" property="og:title">
  <meta content="Kristoffer Waldow, PhD Student" property="description">
  <meta content="Kristoffer Waldow, personal webpage" property="description">


  <script src="https://code.jquery.com/jquery-3.7.1.min.js"
    integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="style.css">

  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap.min.css"
    integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  <!-- Optional theme -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap-theme.min.css"
    integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

  <!-- Latest compiled and minified JavaScript -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"
    integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
    crossorigin="anonymous"></script>

  <script src="https://kit.fontawesome.com/5e613ea1c3.js" crossorigin="anonymous"></script>
</head>

<body onload="setup()">
  <div class="container">
    <div class="row">
      <div class="col-lg-4 col-md-4 col-sm-12">
        <div class="profile">
          <img src="imgs/profile.jpeg" alt="" srcset="">
        </div>
        <div class="sidePanel">
          <div class="contacts">
            <div><i class="fa-solid fa-building-columns"></i> TH K√∂ln | FAU | TUM</div>
            <div><i class="fa-sharp fa-solid fa-location-dot"></i> Cologne, Germany</div>
            <div><a href="mailto:kristoffer.waldow@th-koeln.de"><i class="fa-solid fa-envelope"></i> Mail</a></div>
          </div>
          <div class="links">

            <div><a href="https://scholar.google.de/citations?user=I9nq01cAAAAJ&hl=de" target="_blank"> <i
                  class="fa-brands fa-google-scholar"></i> Google Scholar</a></div>
            <div><a href="https://www.researchgate.net/profile/Kristoffer-Waldow/publications" target="_blank"> <i
                  class="fa-brands fa-researchgate"></i> ResearchGate</a></div>
            <div><a href="https://orcid.org/0000-0002-5176-7530" target="_blank"><i class="fa-brands fa-orcid"></i> ORCID</a></div>
            <div><a href="https://www.linkedin.com/in/kristoffer-waldow/" target="_blank"><i class="fa-brands fa-linkedin"></i>
                LinkedIn</a></div>
            <!-- <div><a href="#"><i class="fa-brands fa-github"></i> Github</a></div> -->
            <div><a href="https://cg.web.th-koeln.de" target="_blank"><i class="fa-solid fa-globe"></i> Computer Graphics Group</a>
            </div>

          </div>

        </div>

      </div>
      <div class="col-lg-8 col-md-8 col-sm-12">
        <h1 class="title">Kristoffer Waldow, M.Sc.</h1>
        <h4>PhD Student @ TH K√∂ln | FAU | TUM</h4>

        <p class="profile-text">
          Hey there! I am Kris, a PhD candidate at the Technical University of Munich, Human-Centered Computing and
          Extended Reality Lab and a research associate at the TH K√∂ln, Computer Graphics research group.
          <br><br>
          I am interested is Mixed Reality (MR) technologies, especially human-computer interaction in MR
          environments to improve interpersonal communication and accessibility with avatars. Currently I am
          researching
          in various funded projects to explore new technologies for human motion capture and sign
          language based on sensor fusion and machine learning.
          <br><br>
          Interested in a research coperation? Feel free to contact me any time via e-mail.
          <br>
          <br>

          <a data-toggle="collapse" href="#moreInfo" role="button" aria-expanded="false" aria-controls="moreInfo">
            <h4 style="text-align: center;">More Infos</h4>
          </a>
        <div class="collapse" id="moreInfo">
          <div>
            As someone with a strong interest in Mixed Reality technologies, I have had the opportunity to delve into
            various projects and research during my studies. My journey began with an internship at the University of
            Cologne, where I worked with a team investigating avatar-based communication in virtual reality (VR). This
            sparked my passion for the field and led me to complete my bachelor's thesis on the creation of a VR
            fishtank with life-size avatars animated in real-time through motion capturing.
            <br><br>
            During my master's studies, I further honed my skills and developed a stage design software with simulated
            3D audio in VR. My final master's thesis allowed me to continue exploring the realm of interpersonal
            communication, this time through avatar-based remote collaboration in augmented reality (AR). I conducted
            two studies to evaluate the effectiveness of this form of collaboration based on avatar representation.
            <br><br>
            Since February 2019, I have been working full-time in scientific research and education, continuing to
            explore the exciting possibilities of Mixed Reality technologies.
            <br><br>
            While being a PhD candidate, I started to explore new technologies for human motion capturing and sign
            language based on sensor fusion and machine learning in different funded projects..
          </div>
        </div>
        <br><br>
        <!-- His field of interest is Mixed Reality (MR) technologies, especially human-computer interaction in MR
            environments to improve interpersonal communication and accessibility with avatars. He is currently
            researching in various funded projects to explore new technologies for human motion capture and sign
            language based on sensor fusion and machine learning. -->
        </p>

        <div class="additional-infos">
          <div>
            <h4>Interesets</h4>
            <ul>
              <li>Mixed Reality</li>
              <li>Avatar-based Communication</li>
              <li>Artificial Intelligence</li>
              <li>Motion Capture</li>
            </ul>
          </div>
          <div>
            <h4>Education</h4>
            <ul class="fa-ul eduction-list">
              <li>
                <span class="fa-li"><i class="fa-solid fa-graduation-cap"></i></span>
                PhD, 2022-now
                <br> <span class="uni">FAU Erlangen / TU Munich / TH K√∂ln</span>
              </li>
              <li>
                <span class="fa-li"><i class="fa-solid fa-graduation-cap"></i></span>
                MSc in Media Technology, 2019
                <br> <span class="uni">Technische Hochschule K√∂ln</span>
              </li>
              <li>
                <span class="fa-li"><i class="fa-solid fa-graduation-cap"></i></span>
                BSc in Media Technology, 2016
                <br> <span class="uni">Technische Hochschule K√∂ln</span>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="news-container">
      <div class="news-content">
        <h4>NEWS</h4>
        <div>
          <li>[03/2024] <span>New Website üéâ But I am not sure who will see this!</span></li>
          <!-- <li>[03/2024] <span>New Website üéâ But I am not sure who will see this!</span></li> -->

        </div>
      </div>
    </div>

    <div class="divider"></div>
    <div class="row">
      <div class="tabs" id="tabs">
        <div class="nav-item nav-active">Publications</div>
        <div class="nav-item">Research Projects</div>
        <div class="nav-item">Teaching</div>
      </div>

      <section id="publications" >
        <div class="year-selection">
          <a href="#2024_FaceEnhance">2024</a>
          <a href="#2022_AVASAG">2022</a>
          <a href="#2021_AVASAG">2021</a>
          <a href="#2020_AdressingDeaf">2020</a>
          <a href="#2019_RemoteCollaboration">2019</a>
          <a href="#2018_Smartphone">2018</a>
          <a href="#2017_SIAMC">2017</a>
          <a href="#2016_SIAMC">2016</a>
        </div>

        <article id="article-template" class="hidden">


          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
              <div class="article-year">2024</div>
              <div class="tags">
                <!-- <span class="poster"></span>
                <span class="full"></span>
                <span class="preprint"></span> -->
              </div>
              <div class="article-teaser-img">
                <img src="" alt="">
              </div>
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12">
              <div class="article-content">

                <div class="article-title">Title
                </div>
                <div class="article-authors">
                  <span>Author A</span>
                </div>
                <div class="article-publication">
                  Conference
                </div>
                <div class="article-links">
                  <a id="abstract-link" data-toggle="collapse" href="#abstract-data-toggle" role="button"
                    aria-expanded="false" aria-controls="abstract-data-toggle">Abstract</a>
                  <div id="additional-links">
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10445598">IEEE</a> -->
                  </div>

                </div>
                <div class="article-abstract">

                  <div class="collapse" id="abstract-data-toggle">
                    Hello World
                  </div>


                </div>
              </div>
            </div>
        </article>

      </section>

      <section id="research-projects" class="hidden">

        <article id="klean">
          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
              <div class="article-year">2023 - now</div>
              <div class="tags">
              </div>
              <div class="article-teaser-img">
                <img
                  src=""
                  alt="">
              </div>
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12">
              <div class="article-content">
  
                <div class="article-title">KLEAN - Analysis of AI-based cleaning of motion capture data for the use of sign language avatars
                </div>
                <div class="article-authors">
                  <span>Charamel GmbH</span>
                  <span>TH K√∂ln</span>
                </div>
                
                <div class="body">
                  <b>In order to overcome the challenges necessary for creating sign language avatars, a prototype software is to be developed as part of an MID innovation voucher that can be used to automatically clean up motion capture data.</b>
                </div>
                <div class="article-links">
                  <div id="additional-links">
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10445598">IEEE</a> -->
                  
                  </div>
  
                </div>
              </div>
            </div>
          </div>
        </article>
        <article id="avasag">
          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
              <div class="article-year">2020 - 2023</div>
              <div class="tags">
              </div>
              <div class="article-teaser-img">
                <img
                  src="https://www.th-koeln.de/mam/bilder/hochschule/fakultaeten/f07/imp/forschung/fittosize_358_201_f9b73a17f386194a3626493944e2adf3_gebardensprach-avatar.jpg"
                  alt="">
              </div>
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12">
              <div class="article-content">
  
                <div class="article-title">Verbundprojekt AVASAG - gemeinsam f√ºr digitale, barrierefreie Kommunikation
                </div>
                <div class="article-authors">
                  <span>Charamel GmbH</span>
                  <span>yomma GmbH</span>
                  <span>Ergosign GmbH</span>
                  <span>DFKI GmbH</span>
                  <span>Universit√§t Augsburg</span>
                  <span>TH K√∂ln</span>
                </div>
                
                <div class="body">
                  <b>
                    Weltweit gibt es etwa 70 Millionen geh√∂rlose Menschen. F√ºr viele von ihnen ist Textsprache eine Fremdsprache; sie bedienen sich stattdessen der Geb√§rdensprache. In dem vom Bundesministerium f√ºr Bildung und Forschung (BMBF) unterst√ºtzten Projekt ‚ÄûAVASAG‚Äú erarbeiten Experten gemeinsam einen 3D-Geb√§rdensprach-Avatar.
                  </b>
                  <br><br>
                  Verbundprojekt AVASAG ‚Äì gemeinsam f√ºr digitale, barrierefreie Kommunikation
                  Geb√§rdensprach-Avatar (Bild: Kristoffer Waldow / TH K√∂ln)
                  Weltweit gibt es etwa 70 Millionen geh√∂rlose Menschen. F√ºr viele von ihnen ist Textsprache eine Fremdsprache; sie bedienen sich stattdessen der Geb√§rdensprache. In dem vom Bundesministerium f√ºr Bildung und Forschung (BMBF) unterst√ºtzten Projekt ‚ÄûAVASAG‚Äú erarbeiten Experten gemeinsam einen 3D-Geb√§rdensprach-Avatar.
                  <br><br>
                  Digitale Kommunikation unterliegt einem schnelllebigen Wandel. Nimmt sie aber jeden mit? Weltweit leben ca. 70 Millionen Geh√∂rlose. F√ºr die meisten ist Textsprache eine Fremdsprache. Der Gesetzgeber hat auf den spezifischen Informationsbedarf von Menschen mit Handicap reagiert und verpflichtet in Deutschland Beh√∂rden und √∂ffentliche Stellen zu barrierefreier digitaler Kommunikation. Digitale Barrierefreiheit wird immer wichtiger, um Inhalte dynamisch und f√ºr alle Zielgruppen richtig aufbereitet zu kommunizieren. Automatisierte Instrumente helfen bei einer barrierefreien Kommunikation.
                  <br><br>
                  Im Projekt wird eine neuartige Geb√§rdenanimations-Methode f√ºr 3D-Avatare geschaffen. Sie kombiniert Methoden des maschinellen Lernens mit regel-basierten Synthesemethoden, die Text in Geb√§rden abbilden. Dabei werden zeitliche und r√§umliche Abh√§ngigkeiten der Geb√§rdenelemente sehr genau aufgel√∂st.
                  <br><br>
                  Das Ziel des Verbundprojekts AVASAG (Avatar-basierter Sprachassistent zur automatisierten Geb√§rden√ºbersetzung) wird ein echtzeitgesteuerter 3D-Geb√§rdensprach-Avatar zur automatischen √úbersetzung deutscher Texte in Geb√§rdensprache sein. Hierdurch soll eine qualitativ realistische Darstellung eines 3D-Geb√§rdensprach-Avatars eine digitale und barrierefreie Kommunikation erm√∂glichen, um so einen Mehrwert f√ºr Geh√∂rlose zu bieten, die besser an der Digitalen Gesellschaft teilhaben k√∂nnen. Unternehmen bietet AVASAG die M√∂glichkeit, Inhalte automatisiert und dynamisch generieren zu k√∂nnen und so die Interaktion und Kommunikation auf digitalen Kan√§len zu optimieren. Als Demonstrator wird ein Geb√§rdensprach-Avatar zur automatisierten √úbersetzung im Bereich Reiseinformation und -service mit Fokus auf Verkehr und Tourismus realisiert.
                  <br><br>
                  Unter Leitung von Charamel arbeiten folgende Forscher und Entwickler aus Deutschland die n√§chsten drei Jahre zusammen: yomma GmbH ‚Äì Experten f√ºr Geb√§rdensprache in Hamburg; Ergosign GmbH, Pionier f√ºr User Experience Design in Saarbr√ºcken; DFKI GmbH ‚Äì Kognitive Assistenzsysteme und Language Technology (DFKI), in Saarbr√ºcken, Technische Hochschule K√∂ln / Institut f√ºr Medien- und Phototechnik, K√∂ln sowie die Universit√§t Augsburg / Human Centered Multimedia (HCM) in Augsburg.
                  <br><br>
                  Die Technische Hochschule K√∂ln bearbeitet das Teilvorhaben "Automatisierte Erfassung von Geb√§rden mittels simultaner Aufnahme von K√∂rper-, Finger-, und Gesichtsbewegungen" und wird dabei folgende Aspekte und technische Herausforderungen im Rahmen des Projektes n√§her untersuchen:
                  <ul>
                    <li>Neuartige Aufnahmetechniken mittels Sensorfusion</li>
                    <li>Umsetzung geeigneter Verfahren zur Abbildung von detaillierten Gesichtsausdr√ºcken</li>
                    <li>Umsetzung und Evaluierung von Verfahren zur synchronen Aufnahme von K√∂rper- und Fingerbewegungen</li>
                    <li>Erstellen eines umfangreichen Motion-Capture Datensatzes zur Animation eines 3D-Geb√§rdensprach-Avatars</li>
                  </ul>
  
  
                </div>
                <div class="article-links">
                  <div id="additional-links">
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10445598">IEEE</a> -->
                    <a
                      href="https://www.th-koeln.de/informations-medien-und-elektrotechnik/verbundprojekt-avasag_76412.php">Zur Projektseite</a>
                  </div>
  
                </div>
              </div>
            </div>
          </div>
        </article>

        <article id="retail">
        <div class="row">
          <div class="col-lg-4 col-md-4 col-sm-12 text-center">
            <div class="article-year">2017 - 2020</div>
            <div class="tags">
            </div>
            <div class="article-teaser-img">
              <img
                src="https://www.th-koeln.de/mam/bilder/hochschule/fakultaeten/f07/imp/forschung/resize__358_201_on_031b9ecfa578cd6afcb5705160bdb421_feedback.jpg"
                alt="">
            </div>
          </div>
          <div class="col-lg-8 col-md-8 col-sm-12">
            <div class="article-content">

              <div class="article-title">Forschungsprojekt Retail 4.0 - AR und VR L√∂sungen f√ºr die Modebranche
              </div>
              <div class="article-authors">
                <span>Assyst GmbH</span>
                <span>Brax Leineweber GmbH &Co. KG</span>
                <span>Human Solutions GmbH</span>
                <span>Deutsche Institute f√ºr Textil und Faserforschung</span>
                <span>TH K√∂ln</span>
              </div>
              
              <div class="body">
                <b>
                  Das Retail 4.0 Projekt entwickelt AR/VR L√∂sungen f√ºr die Modebranche zur Einbindung von Einzelh√§ndlern
                  und Endkunden in den Feedbackprozess w√§hrend der Kollektionsentwicklung. Zu den besonderen technischen
                  Herausforderungen geh√∂ren das realistische Rendering und die akkurate Farbwiedergabe der
                  Kleidungsst√ºcke auf mobilen VR/AR-Displays.
                </b>
                <br><br>
                Digitale Technologien erobern die Modebranche und er√∂ffnen neue M√∂glichkeiten der Einflussnahme auf die
                Kollektionsentwicklung durch Einzelhandel und Endkunden.
                <br><br>
                Ziel des Forschungsprojekts Retail 4.0 ist die Entwicklung einer modularen skalierbaren Softwarel√∂sung,
                die eine zeitnahe Weitergabe von Feedback des Einzelhandels zur Kollektionsentwicklung an den Hersteller
                erm√∂glicht und den Kunden √ºber ein neues, digitales VR/AR Shoppingerlebnis direkt mit einbindet.
                <br><br>
                Um dieses Ziel zu erreichen, wird auf der Basis einer heute eingesetzten L√∂sung f√ºr 3D-Simulation und
                Visualisierung eine VR/AR-Anwendung mit Cloud-Anbindung entwickelt, die auch mobile Endger√§te
                unterst√ºtzt. Hierzu werden innovative Prozesse zur Kommunikation zwischen dem Einzelh√§ndler und dem
                Hersteller konzipiert und implementiert. F√ºr den Fall der Verf√ºgbarkeit der Kollektion im Gesch√§ft
                werden Methoden f√ºr ein effizientes Feedback des Endkunden an den Hersteller eingebunden. Virtuelle
                Produkte sollen vom Endkunden auch zur Kommunikation in sozialen Netzwerken genutzt werden k√∂nnen. Die
                L√∂sung wird dem Einzelh√§ndler als Dienstleistung angeboten, der es dann wiederum den Endkunden zur
                Verf√ºgung stellt.
                <br><br>
                Die prototypische Umsetzung der L√∂sung erfolgt im Entwicklungszentrum des projektbegleitenden
                Herstellers unter Einbindung ausgew√§hlter Einzelh√§ndler.
                <br><br>
                Die L√∂sungen von Retail 4.0 erm√∂glichen Bekleidungseinzelh√§ndlern, nennenswerten Einfluss auf ihr
                zuk√ºnftiges Sortiment bereits in einer fr√ºhen Phase der Entwicklung zu nehmen sowie den Kunden ein
                einzigartiges Shoppingerlebnis zu vermitteln. Insbesondere die Mechanismen f√ºr
                User-Community-Interaktion f√ºhren zur Nutzbarmachung neuer Kreativpotenziale, einer erh√∂hten
                Kundenbindung und somit signifikanten Wettbewerbsvorteilen.
                <br><br>

                Folgende Aspekte und technischen Herausforderungen werden im Rahmen des Projekts beleuchtet und n√§her
                untersucht:
                <ul>
                  <li> lebensechte virtuelle Prototypen der Bekleidungsst√ºcke</li>
                  <li>neue Verfahren f√ºr ein Echtzeitrendering von Kleidungsst√ºcken in 3D auf mobilen Ger√§ten</li>
                  <li>neue Verfahren f√ºr die akkurate Farbwiedergabe auf mobilen Ger√§ten</li>
                  <li>Identifizierung der richtigen Passform f√ºr den Kunden</li>
                  <li>Diskussion von Farben, Kollektionsthemen, Materialauswahl, Schnitten und Kollektionselementen mit
                    Hilfe von Annotationen</li>
                  <li>Interaktion und Kollaboration der Nutzer untereinander</li>
                  <li>Ermittlung von Kundenverhalten und Pr√§ferenzen</li>
                </ul>


              </div>
              <div class="article-links">
                <div id="additional-links">
                  <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10445598">IEEE</a> -->
                  <a
                    href="https://www.th-koeln.de/informations-medien-und-elektrotechnik/forschungsprojekt-retail-40_50232.php">Zur Projektseite</a>
                </div>

              </div>
            </div>
          </div>
        </div>
      </article>

      </section>
      <section id="teaching" class="hidden">
        <article>
          <p>
            Asisstant teaching at the Institute for Media and Photo Technology (IMP), TH K√∂ln <br><br>
            
            <b>Own courses:</b>
            <ul>
              <li>WEB1 - Backend Technologies (Bachelor)</li>
              <li>WEB2 - Frontend Technologies (Bachelor)</li>
            </ul>
            
            <b>Modules:</b>
            <ul>
              <li>Computer Science (Bachelor/Masters)</li>
              <li>Computer Graphics (Bachelor)</li>
              <li>Parallel Computing on CPU and GPU (Masters)</li>
            </ul>
            <br>
            <b>Additional Tasks:</b>
            <ul>
              <li>Supervision of the motion capture studio</li>
            <li>Supervision of Bachelor's and Master's theses</li>
            </ul>
          </p>
        </article>
      </section>
    </div>
  </div>
  <footer> Copyright @ Kristoffer Waldow | This is my personal webpage.</footer>


  <script>

    async function setup() {

      const article_template = document.getElementById("article-template");

      const jsonArticles = await fetch("files/articles.json");
      const data = await jsonArticles.json();

      for (let i = 0; i < data.articles.length; i++) {
        const article = data.articles[i];
        const id = article.id;
        //console.log(article.links)

        let newArticle = article_template.cloneNode(true);
        document.getElementById("publications").appendChild(newArticle);
        newArticle.classList.remove("hidden");

        newArticle.id = article.id;

        newArticle.querySelector('.article-year').textContent = article.year;
        newArticle.querySelector('.article-title').textContent = article.title;
        newArticle.querySelector('.article-authors').innerHTML = article.authors.map(author => `<span>${(author.trim()== "Kristoffer Waldow")?"<b>Kristoffer Waldow</b>":author.trim()}</span>`).join('');
        newArticle.querySelector('.article-publication').textContent = article.published;

        newArticle.querySelector('.tags').innerHTML = article.tags.map(tag => `<span class=${tag}></span>`).join('');

        const abstractID = "abstract_" + id;
        newArticle.querySelector('#abstract-link').href = "#" + abstractID;
        newArticle.querySelector('#abstract-link').setAttribute("aria-controls", abstractID);

        newArticle.querySelector('#abstract-data-toggle').textContent = article.abstract;
        newArticle.querySelector('#abstract-data-toggle').id = abstractID;

        let linkContainer = newArticle.querySelector("#additional-links");
        article.links.map(item => {
          let newLink = document.createElement("a");
          newLink.href = item.link;
          newLink.textContent = item.name;
          newLink.target ="_blank";
          linkContainer.appendChild(newLink);
        })

        let img = newArticle.querySelector("img");
        img.src = article.image;
      }


      const children = document.getElementById("tabs").children;
      const sections = document.getElementsByTagName("section");
      for (let i = 0; i < children.length; i++) {
        const index = i;
        children[i].addEventListener("click", () => { changeTab(index) })
      }

      function changeTab(id) {
        for (let i = 0; i < children.length; i++) {
          children[i].classList.remove("nav-active");
        }
        children[id].classList.add("nav-active");

        for (let i = 0; i < sections.length; i++) {
          sections[i].classList.add("hidden");
        }
        sections[id].classList.remove("hidden");
      }
    }


  </script>
</body>

</html>