<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />

  <title>Kristoffer Waldow</title>

  <meta name="author" content="Kristoffer Waldow">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta content="Kristoffer Waldow, PhD Student" property="og:title">
  <meta content="Kristoffer Waldow, PhD Student" property="description">
  <meta content="Kristoffer Waldow, personal webpage" property="description">


  <script src="https://code.jquery.com/jquery-3.7.1.min.js"
    integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="style.css">

  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap.min.css"
    integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  <!-- Optional theme -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap-theme.min.css"
    integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

  <!-- Latest compiled and minified JavaScript -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"
    integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
    crossorigin="anonymous"></script>

  <script src="https://kit.fontawesome.com/5e613ea1c3.js" crossorigin="anonymous"></script>
</head>

<body onload="setup()">
  <div class="container">
    <div class="row">
      <div class="col-lg-4 col-md-4 col-sm-12">
        <div class="profile">
          <img src="imgs/profile.jpeg" alt="" srcset="">
        </div>
        <div class="sidePanel">
          <div class="contacts">
            <div><i class="fa-solid fa-building-columns"></i> TH KÃ¶ln | FAU | TUM</div>
            <div><i class="fa-sharp fa-solid fa-location-dot"></i> Cologne, Germany</div>
            <div><a href="mailto:kristoffer.waldow@th-koeln.de"><i class="fa-solid fa-envelope"></i> Mail</a></div>
          </div>
          <div class="links">

            <div><a href="https://scholar.google.de/citations?user=I9nq01cAAAAJ&hl=de" target="_blank"> <i
                  class="fa-brands fa-google-scholar"></i> Google Scholar</a></div>
            <div><a href="https://www.researchgate.net/profile/Kristoffer-Waldow/publications" target="_blank"> <i
                  class="fa-brands fa-researchgate"></i> ResearchGate</a></div>
            <div><a href="https://orcid.org/0000-0002-5176-7530" target="_blank"><i class="fa-brands fa-orcid"></i> ORCID</a></div>
            <div><a href="https://www.linkedin.com/in/kristoffer-waldow/" target="_blank"><i class="fa-brands fa-linkedin"></i>
                LinkedIn</a></div>
            <!-- <div><a href="#"><i class="fa-brands fa-github"></i> Github</a></div> -->
            <div><a href="https://cg.web.th-koeln.de" target="_blank"><i class="fa-solid fa-globe"></i> Computer Graphics Group</a>
            </div>

          </div>

        </div>

      </div>
      <div class="col-lg-8 col-md-8 col-sm-12">
        <h1 class="title">Kristoffer Waldow, M.Sc.</h1>
        <h4>PhD Student @ TH KÃ¶ln | FAU | TUM</h4>

        <p class="profile-text">
          Hey there! I am Kris, a PhD candidate at the Technical University of Munich, Human-Centered Computing and
          Extended Reality Lab and a research associate at the TH KÃ¶ln, Computer Graphics research group.
          <br><br>
          I am interested is Mixed Reality (MR) technologies, especially human-computer interaction in MR
          environments to improve interpersonal communication and accessibility with avatars. Currently I am
          researching
          in various funded projects to explore new technologies for human motion capture and sign
          language based on sensor fusion and machine learning.
          <br><br>
          Interested in a research coperation? Feel free to contact me any time via e-mail.
          <br>
          <br>

          <a data-toggle="collapse" href="#moreInfo" role="button" aria-expanded="false" aria-controls="moreInfo">
            <h4 style="text-align: center;">More Infos</h4>
          </a>
        <div class="collapse" id="moreInfo">
          <div>
            As someone with a strong interest in Mixed Reality technologies, I have had the opportunity to delve into
            various projects and research during my studies. My journey began with an internship at the University of
            Cologne, where I worked with a team investigating avatar-based communication in virtual reality (VR). This
            sparked my passion for the field and led me to complete my bachelor's thesis on the creation of a VR
            fishtank with life-size avatars animated in real-time through motion capturing.
            <br><br>
            During my master's studies, I further honed my skills and developed a stage design software with simulated
            3D audio in VR. My final master's thesis allowed me to continue exploring the realm of interpersonal
            communication, this time through avatar-based remote collaboration in augmented reality (AR). I conducted
            two studies to evaluate the effectiveness of this form of collaboration based on avatar representation.
            <br><br>
            Since February 2019, I have been working full-time in scientific research and education, continuing to
            explore the exciting possibilities of Mixed Reality technologies.
            <br><br>
            While being a PhD candidate, I started to explore new technologies for human motion capturing and sign
            language based on sensor fusion and machine learning in different funded projects..
          </div>
        </div>
        <br><br>
        <!-- His field of interest is Mixed Reality (MR) technologies, especially human-computer interaction in MR
            environments to improve interpersonal communication and accessibility with avatars. He is currently
            researching in various funded projects to explore new technologies for human motion capture and sign
            language based on sensor fusion and machine learning. -->
        </p>

        <div class="additional-infos">
          <div>
            <h4>Interesets</h4>
            <ul>
              <li>Mixed Reality</li>
              <li>Avatar-based Communication</li>
              <li>Artificial Intelligence</li>
              <li>Motion Capture</li>
            </ul>
          </div>
          <div>
            <h4>Education</h4>
            <ul class="fa-ul eduction-list">
              <li>
                <span class="fa-li"><i class="fa-solid fa-graduation-cap"></i></span>
                PhD, 2022-now
                <br> <span class="uni">FAU Erlangen / TU Munich / TH KÃ¶ln</span>
              </li>
              <li>
                <span class="fa-li"><i class="fa-solid fa-graduation-cap"></i></span>
                MSc in Media Technology, 2019
                <br> <span class="uni">Technische Hochschule KÃ¶ln</span>
              </li>
              <li>
                <span class="fa-li"><i class="fa-solid fa-graduation-cap"></i></span>
                BSc in Media Technology, 2016
                <br> <span class="uni">Technische Hochschule KÃ¶ln</span>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="news-container">
      <div class="news-content">
        <h4>NEWS</h4>
        <div>
          <li>[03/2024] <span>New Website ðŸŽ‰ But I am not sure who will see this!</span></li>
          <!-- <li>[03/2024] <span>New Website ðŸŽ‰ But I am not sure who will see this!</span></li> -->

        </div>
      </div>
    </div>

    <div class="divider"></div>
    <div class="row">
      <div class="tabs" id="tabs">
        <div class="nav-item nav-active">Publications</div>
        <div class="nav-item">Research Projects</div>
        <div class="nav-item">Teaching</div>
      </div>

      <section id="publications" >
        <div class="year-selection">
          <a href="#2024_FaceEnhance">2024</a>
          <a href="#2022_AVASAG">2022</a>
          <a href="#2021_AVASAG">2021</a>
          <a href="#2020_AdressingDeaf">2020</a>
          <a href="#2019_RemoteCollaboration">2019</a>
          <a href="#2018_Smartphone">2018</a>
          <a href="#2017_SIAMC">2017</a>
          <a href="#2016_SIAMC">2016</a>
        </div>

        <article id="article-template" class="hidden">


          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
              <div class="article-year">2024</div>
              <div class="tags">
                <!-- <span class="poster"></span>
                <span class="full"></span>
                <span class="preprint"></span> -->
              </div>
              <div class="article-teaser-img">
                <img src="" alt="">
              </div>
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12">
              <div class="article-content">

                <div class="article-title">Title
                </div>
                <div class="article-authors">
                  <span>Author A</span>
                </div>
                <div class="article-publication">
                  Conference
                </div>
                <div class="article-links">
                  <a id="abstract-link" data-toggle="collapse" href="#abstract-data-toggle" role="button"
                    aria-expanded="false" aria-controls="abstract-data-toggle">Abstract</a>
                  <div id="additional-links">
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10445598">IEEE</a> -->
                  </div>

                </div>
                <div class="article-abstract">

                  <div class="collapse" id="abstract-data-toggle">
                    Hello World
                  </div>


                </div>
              </div>
            </div>
        </article>

      </section>

      <section id="research-projects" class="hidden">

        <article id="klean">
          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
              <div class="article-year">2023 - now</div>
              <div class="tags">
              </div>
              <div class="article-teaser-img">
                <img
                  src=""
                  alt="">
              </div>
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12">
              <div class="article-content">
  
                <div class="article-title">KLEAN - Analysis of AI-based cleaning of motion capture data for the use of sign language avatars
                </div>
                <div class="article-authors">
                  <span>Charamel GmbH</span>
                  <span>TH KÃ¶ln</span>
                </div>
                
                <div class="body">
                  <b>In order to overcome the challenges necessary for creating sign language avatars, a prototype software is to be developed as part of an MID innovation voucher that can be used to automatically clean up motion capture data.</b>
                </div>
                <div class="article-links">
                  <div id="additional-links">
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10445598">IEEE</a> -->
                  
                  </div>
  
                </div>
              </div>
            </div>
          </div>
        </article>
        <article id="avasag">
          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
              <div class="article-year">2020 - 2023</div>
              <div class="tags">
              </div>
              <div class="article-teaser-img">
                <img
                  src="https://www.th-koeln.de/mam/bilder/hochschule/fakultaeten/f07/imp/forschung/fittosize_358_201_f9b73a17f386194a3626493944e2adf3_gebardensprach-avatar.jpg"
                  alt="">
              </div>
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12">
              <div class="article-content">
  
                <div class="article-title">Verbundprojekt AVASAG - gemeinsam fÃ¼r digitale, barrierefreie Kommunikation
                </div>
                <div class="article-authors">
                  <span>Charamel GmbH</span>
                  <span>yomma GmbH</span>
                  <span>Ergosign GmbH</span>
                  <span>DFKI GmbH</span>
                  <span>UniversitÃ¤t Augsburg</span>
                  <span>TH KÃ¶ln</span>
                </div>
                
                <div class="body">
                  <b>
                    Weltweit gibt es etwa 70 Millionen gehÃ¶rlose Menschen. FÃ¼r viele von ihnen ist Textsprache eine Fremdsprache; sie bedienen sich stattdessen der GebÃ¤rdensprache. In dem vom Bundesministerium fÃ¼r Bildung und Forschung (BMBF) unterstÃ¼tzten Projekt â€žAVASAGâ€œ erarbeiten Experten gemeinsam einen 3D-GebÃ¤rdensprach-Avatar.
                  </b>
                  <br><br>
                  Verbundprojekt AVASAG â€“ gemeinsam fÃ¼r digitale, barrierefreie Kommunikation
                  GebÃ¤rdensprach-Avatar (Bild: Kristoffer Waldow / TH KÃ¶ln)
                  Weltweit gibt es etwa 70 Millionen gehÃ¶rlose Menschen. FÃ¼r viele von ihnen ist Textsprache eine Fremdsprache; sie bedienen sich stattdessen der GebÃ¤rdensprache. In dem vom Bundesministerium fÃ¼r Bildung und Forschung (BMBF) unterstÃ¼tzten Projekt â€žAVASAGâ€œ erarbeiten Experten gemeinsam einen 3D-GebÃ¤rdensprach-Avatar.
                  <br><br>
                  Digitale Kommunikation unterliegt einem schnelllebigen Wandel. Nimmt sie aber jeden mit? Weltweit leben ca. 70 Millionen GehÃ¶rlose. FÃ¼r die meisten ist Textsprache eine Fremdsprache. Der Gesetzgeber hat auf den spezifischen Informationsbedarf von Menschen mit Handicap reagiert und verpflichtet in Deutschland BehÃ¶rden und Ã¶ffentliche Stellen zu barrierefreier digitaler Kommunikation. Digitale Barrierefreiheit wird immer wichtiger, um Inhalte dynamisch und fÃ¼r alle Zielgruppen richtig aufbereitet zu kommunizieren. Automatisierte Instrumente helfen bei einer barrierefreien Kommunikation.
                  <br><br>
                  Im Projekt wird eine neuartige GebÃ¤rdenanimations-Methode fÃ¼r 3D-Avatare geschaffen. Sie kombiniert Methoden des maschinellen Lernens mit regel-basierten Synthesemethoden, die Text in GebÃ¤rden abbilden. Dabei werden zeitliche und rÃ¤umliche AbhÃ¤ngigkeiten der GebÃ¤rdenelemente sehr genau aufgelÃ¶st.
                  <br><br>
                  Das Ziel des Verbundprojekts AVASAG (Avatar-basierter Sprachassistent zur automatisierten GebÃ¤rdenÃ¼bersetzung) wird ein echtzeitgesteuerter 3D-GebÃ¤rdensprach-Avatar zur automatischen Ãœbersetzung deutscher Texte in GebÃ¤rdensprache sein. Hierdurch soll eine qualitativ realistische Darstellung eines 3D-GebÃ¤rdensprach-Avatars eine digitale und barrierefreie Kommunikation ermÃ¶glichen, um so einen Mehrwert fÃ¼r GehÃ¶rlose zu bieten, die besser an der Digitalen Gesellschaft teilhaben kÃ¶nnen. Unternehmen bietet AVASAG die MÃ¶glichkeit, Inhalte automatisiert und dynamisch generieren zu kÃ¶nnen und so die Interaktion und Kommunikation auf digitalen KanÃ¤len zu optimieren. Als Demonstrator wird ein GebÃ¤rdensprach-Avatar zur automatisierten Ãœbersetzung im Bereich Reiseinformation und -service mit Fokus auf Verkehr und Tourismus realisiert.
                  <br><br>
                  Unter Leitung von Charamel arbeiten folgende Forscher und Entwickler aus Deutschland die nÃ¤chsten drei Jahre zusammen: yomma GmbH â€“ Experten fÃ¼r GebÃ¤rdensprache in Hamburg; Ergosign GmbH, Pionier fÃ¼r User Experience Design in SaarbrÃ¼cken; DFKI GmbH â€“ Kognitive Assistenzsysteme und Language Technology (DFKI), in SaarbrÃ¼cken, Technische Hochschule KÃ¶ln / Institut fÃ¼r Medien- und Phototechnik, KÃ¶ln sowie die UniversitÃ¤t Augsburg / Human Centered Multimedia (HCM) in Augsburg.
                  <br><br>
                  Die Technische Hochschule KÃ¶ln bearbeitet das Teilvorhaben "Automatisierte Erfassung von GebÃ¤rden mittels simultaner Aufnahme von KÃ¶rper-, Finger-, und Gesichtsbewegungen" und wird dabei folgende Aspekte und technische Herausforderungen im Rahmen des Projektes nÃ¤her untersuchen:
                  <ul>
                    <li>Neuartige Aufnahmetechniken mittels Sensorfusion</li>
                    <li>Umsetzung geeigneter Verfahren zur Abbildung von detaillierten GesichtsausdrÃ¼cken</li>
                    <li>Umsetzung und Evaluierung von Verfahren zur synchronen Aufnahme von KÃ¶rper- und Fingerbewegungen</li>
                    <li>Erstellen eines umfangreichen Motion-Capture Datensatzes zur Animation eines 3D-GebÃ¤rdensprach-Avatars</li>
                  </ul>
  
  
                </div>
                <div class="article-links">
                  <div id="additional-links">
                    <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10445598">IEEE</a> -->
                    <a
                      href="https://www.th-koeln.de/informations-medien-und-elektrotechnik/verbundprojekt-avasag_76412.php">Zur Projektseite</a>
                  </div>
  
                </div>
              </div>
            </div>
          </div>
        </article>

        <article id="retail">
        <div class="row">
          <div class="col-lg-4 col-md-4 col-sm-12 text-center">
            <div class="article-year">2017 - 2020</div>
            <div class="tags">
            </div>
            <div class="article-teaser-img">
              <img
                src="https://www.th-koeln.de/mam/bilder/hochschule/fakultaeten/f07/imp/forschung/resize__358_201_on_031b9ecfa578cd6afcb5705160bdb421_feedback.jpg"
                alt="">
            </div>
          </div>
          <div class="col-lg-8 col-md-8 col-sm-12">
            <div class="article-content">

              <div class="article-title">Forschungsprojekt Retail 4.0 - AR und VR LÃ¶sungen fÃ¼r die Modebranche
              </div>
              <div class="article-authors">
                <span>Assyst GmbH</span>
                <span>Brax Leineweber GmbH &Co. KG</span>
                <span>Human Solutions GmbH</span>
                <span>Deutsche Institute fÃ¼r Textil und Faserforschung</span>
                <span>TH KÃ¶ln</span>
              </div>
              
              <div class="body">
                <b>
                  Das Retail 4.0 Projekt entwickelt AR/VR LÃ¶sungen fÃ¼r die Modebranche zur Einbindung von EinzelhÃ¤ndlern
                  und Endkunden in den Feedbackprozess wÃ¤hrend der Kollektionsentwicklung. Zu den besonderen technischen
                  Herausforderungen gehÃ¶ren das realistische Rendering und die akkurate Farbwiedergabe der
                  KleidungsstÃ¼cke auf mobilen VR/AR-Displays.
                </b>
                <br><br>
                Digitale Technologien erobern die Modebranche und erÃ¶ffnen neue MÃ¶glichkeiten der Einflussnahme auf die
                Kollektionsentwicklung durch Einzelhandel und Endkunden.
                <br><br>
                Ziel des Forschungsprojekts Retail 4.0 ist die Entwicklung einer modularen skalierbaren SoftwarelÃ¶sung,
                die eine zeitnahe Weitergabe von Feedback des Einzelhandels zur Kollektionsentwicklung an den Hersteller
                ermÃ¶glicht und den Kunden Ã¼ber ein neues, digitales VR/AR Shoppingerlebnis direkt mit einbindet.
                <br><br>
                Um dieses Ziel zu erreichen, wird auf der Basis einer heute eingesetzten LÃ¶sung fÃ¼r 3D-Simulation und
                Visualisierung eine VR/AR-Anwendung mit Cloud-Anbindung entwickelt, die auch mobile EndgerÃ¤te
                unterstÃ¼tzt. Hierzu werden innovative Prozesse zur Kommunikation zwischen dem EinzelhÃ¤ndler und dem
                Hersteller konzipiert und implementiert. FÃ¼r den Fall der VerfÃ¼gbarkeit der Kollektion im GeschÃ¤ft
                werden Methoden fÃ¼r ein effizientes Feedback des Endkunden an den Hersteller eingebunden. Virtuelle
                Produkte sollen vom Endkunden auch zur Kommunikation in sozialen Netzwerken genutzt werden kÃ¶nnen. Die
                LÃ¶sung wird dem EinzelhÃ¤ndler als Dienstleistung angeboten, der es dann wiederum den Endkunden zur
                VerfÃ¼gung stellt.
                <br><br>
                Die prototypische Umsetzung der LÃ¶sung erfolgt im Entwicklungszentrum des projektbegleitenden
                Herstellers unter Einbindung ausgewÃ¤hlter EinzelhÃ¤ndler.
                <br><br>
                Die LÃ¶sungen von Retail 4.0 ermÃ¶glichen BekleidungseinzelhÃ¤ndlern, nennenswerten Einfluss auf ihr
                zukÃ¼nftiges Sortiment bereits in einer frÃ¼hen Phase der Entwicklung zu nehmen sowie den Kunden ein
                einzigartiges Shoppingerlebnis zu vermitteln. Insbesondere die Mechanismen fÃ¼r
                User-Community-Interaktion fÃ¼hren zur Nutzbarmachung neuer Kreativpotenziale, einer erhÃ¶hten
                Kundenbindung und somit signifikanten Wettbewerbsvorteilen.
                <br><br>

                Folgende Aspekte und technischen Herausforderungen werden im Rahmen des Projekts beleuchtet und nÃ¤her
                untersucht:
                <ul>
                  <li> lebensechte virtuelle Prototypen der BekleidungsstÃ¼cke</li>
                  <li>neue Verfahren fÃ¼r ein Echtzeitrendering von KleidungsstÃ¼cken in 3D auf mobilen GerÃ¤ten</li>
                  <li>neue Verfahren fÃ¼r die akkurate Farbwiedergabe auf mobilen GerÃ¤ten</li>
                  <li>Identifizierung der richtigen Passform fÃ¼r den Kunden</li>
                  <li>Diskussion von Farben, Kollektionsthemen, Materialauswahl, Schnitten und Kollektionselementen mit
                    Hilfe von Annotationen</li>
                  <li>Interaktion und Kollaboration der Nutzer untereinander</li>
                  <li>Ermittlung von Kundenverhalten und PrÃ¤ferenzen</li>
                </ul>


              </div>
              <div class="article-links">
                <div id="additional-links">
                  <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10445598">IEEE</a> -->
                  <a
                    href="https://www.th-koeln.de/informations-medien-und-elektrotechnik/forschungsprojekt-retail-40_50232.php">Zur Projektseite</a>
                </div>

              </div>
            </div>
          </div>
        </div>
      </article>

      </section>
      <section id="teaching" class="hidden">
        <article>
          <p>
            Asisstant teaching at the Institute for Media and Photo Technology (IMP), TH KÃ¶ln <br><br>
            
            <b>Own courses:</b>
            <ul>
              <li>WEB1 - Backend Technologies (Bachelor)</li>
              <li>WEB2 - Frontend Technologies (Bachelor)</li>
            </ul>
            
            <b>Modules:</b>
            <ul>
              <li>Computer Science (Bachelor/Masters)</li>
              <li>Computer Graphics (Bachelor)</li>
              <li>Parallel Computing on CPU and GPU (Masters)</li>
            </ul>
            <br>
            <b>Additional Tasks:</b>
            <ul>
              <li>Supervision of the motion capture studio</li>
            <li>Supervision of Bachelor's and Master's theses</li>
            </ul>
          </p>
        </article>
      </section>
    </div>
  </div>
  <footer> Copyright @ Kristoffer Waldow | This is my personal webpage.</footer>


  <script>

    async function setup() {

      const article_template = document.getElementById("article-template");

      const jsonArticles = await fetch("files/articles.json");
      const data = await jsonArticles.json();

      for (let i = 0; i < data.articles.length; i++) {
        const article = data.articles[i];
        const id = article.id;
        //console.log(article.links)

        let newArticle = article_template.cloneNode(true);
        document.getElementById("publications").appendChild(newArticle);
        newArticle.classList.remove("hidden");

        newArticle.id = article.id;

        newArticle.querySelector('.article-year').textContent = article.year;
        newArticle.querySelector('.article-title').textContent = article.title;
        newArticle.querySelector('.article-authors').innerHTML = article.authors.map(author => `<span>${(author.trim()== "Kristoffer Waldow")?"<b>Kristoffer Waldow</b>":author.trim()}</span>`).join('');
        newArticle.querySelector('.article-publication').textContent = article.published;

        newArticle.querySelector('.tags').innerHTML = article.tags.map(tag => `<span class=${tag}></span>`).join('');

        const abstractID = "abstract_" + id;
        newArticle.querySelector('#abstract-link').href = "#" + abstractID;
        newArticle.querySelector('#abstract-link').setAttribute("aria-controls", abstractID);

        newArticle.querySelector('#abstract-data-toggle').textContent = article.abstract;
        newArticle.querySelector('#abstract-data-toggle').id = abstractID;

        let linkContainer = newArticle.querySelector("#additional-links");
        article.links.map(item => {
          let newLink = document.createElement("a");
          newLink.href = item.link;
          newLink.textContent = item.name;
          newLink.target ="_blank";
          linkContainer.appendChild(newLink);
        })

        let img = newArticle.querySelector("img");
        img.src = article.image;
      }


      const children = document.getElementById("tabs").children;
      const sections = document.getElementsByTagName("section");
      for (let i = 0; i < children.length; i++) {
        const index = i;
        children[i].addEventListener("click", () => { changeTab(index) })
      }

      function changeTab(id) {
        for (let i = 0; i < children.length; i++) {
          children[i].classList.remove("nav-active");
        }
        children[id].classList.add("nav-active");

        for (let i = 0; i < sections.length; i++) {
          sections[i].classList.add("hidden");
        }
        sections[id].classList.remove("hidden");
      }
    }


  </script>
</body>

</html>